{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my second Kaggle Competition I have learned a lot from the First which was \"Titanic\" I would suggest the beginners to go through Intro. and Intermediate ML course and apply those things in the Titanic Competition before doing this one.\n\nIn this Competition I try to make things more clear, precise and organise and perform better feature engineering than the Titanic Competition.\n\n- Various libraries and use of scikit-learn"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":"### Libraries and use of scikit-learn"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import train_test_split\nimport itertools\nfrom sklearn.metrics import mean_squared_error\nimport category_encoders ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loading data and extracting features\nHere We will extract the columns or features who has missing values less than 800 and we will find out that intially we had 81 columns and after this operation we would left with 76 columns, 5 columns has missing values less than 800."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/train.csv')\ntest_data=pd.read_csv('/kaggle/input/house-prices-advanced-regression-techniques/test.csv')\nprint(train_data.info())\nfeatures=[cols for cols in train_data.columns if(train_data[cols].isnull().sum()<800)]\n#we have choose 800 because fireplacequ has values 770 and we don't want to lose that information \n\nX=train_data[features]\ny=X.SalePrice\nX.drop(columns={'SalePrice'},inplace=True)\nX_test=test_data[X.columns]\nX.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering\n We have 76 columns and now we will preprocess all of these,handling categorical and numerical data and fill the missing values to make it ready for Machine learning models.\n - Categorical data and missing values\n - numerical data and missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_id=X_test.Id\nX.drop(columns={'Id'},inplace=True)\nX_test.drop(columns={'Id'},inplace=True)\ncat_col=[col for col in X.columns if X[col].dtype=='object']\nnum_col=[col for col in X.columns if X[col].dtype!='object']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_c=X.copy()\ny_c=y.copy()\nX_test_c=X_test.copy()\nencoder=LabelEncoder()\nfor col in cat_col:\n    X_c[col]=X_c[col].astype(str)\n    X_test_c[col]=X_test_c[col].astype(str)\n    X_c[col]=encoder.fit_transform(X_c[col])\n    X_test_c[col]=encoder.fit_transform(X_test_c[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace the missing value with mode of that column and encoding by Label encoder\nX_c1=X.copy()\ny_c1=y.copy()\nX_test_c1=X_test.copy()\nmissing_catcol=[col for col in X_c1[cat_col].columns if X_c1[col].isnull().any()==True]\nX_c1[missing_catcol]=X_c1[missing_catcol].fillna(X_c1[missing_catcol].mode().iloc[0])\nmissing_catcol=[col for col in X_test_c1[cat_col].columns if X_test_c1[col].isnull().any()==True]\nX_test_c1[missing_catcol]=X_test_c1[missing_catcol].fillna(X_test_c1[missing_catcol].mode().iloc[0])\nfor col in cat_col:\n    X_c1[col]=encoder.fit_transform(X_c1[col])\n    X_test_c1[col]=encoder.fit_transform(X_test_c1[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Replace the missing value with mode of that column and encoding by Count Encoder\nX_c2=X.copy()\ny_c2=y.copy()\nX_test_c2=X_test.copy()\nmissing_catcol=[col for col in X_c2[cat_col].columns if X_c2[col].isnull().any()==True]\nX_c2[missing_catcol]=X_c2[missing_catcol].fillna(X_c2[missing_catcol].mode().iloc[0])\nmissing_catcol=[col for col in X_test_c1[cat_col].columns if X_test_c1[col].isnull().any()==True]\nX_test_c1[missing_catcol]=X_test_c1[missing_catcol].fillna(X_test_c1[missing_catcol].mode().iloc[0])\nencoder2=category_encoders.CountEncoder()\nX_c2[cat_col]=encoder2.fit_transform(X_c2[cat_col])\nX_test_c2[cat_col]=encoder2.transform(X_test_c2[cat_col])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical data\nimputer=SimpleImputer()\nX_c[num_col]=pd.DataFrame(imputer.fit_transform(X_c1[num_col]))\nX_test_c1[num_col]=pd.DataFrame(imputer.transform(X_test_c1[num_col]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical data\nX_c1[num_col]=pd.DataFrame(imputer.fit_transform(X_c[num_col]))\nX_test_c[num_col]=pd.DataFrame(imputer.transform(X_test_c[num_col]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#numerical data\nX_c2[num_col]=pd.DataFrame(imputer.fit_transform(X_c2[num_col]))\nX_test_c2[num_col]=pd.DataFrame(imputer.transform(X_test_c2[num_col]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Selection\n- Random Forest\n- XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Random Forest on Label encoder and Simple Imputer\nX_train,X_valid,y_train,y_valid=train_test_split(X_c,y_c)\nmodel=RandomForestRegressor()\nmodel.fit(X_train,y_train)\n#print prediction for both training and validation data to get the idea of overfitting.\nprint(mean_absolute_error(model.predict(X_valid),y_valid))\nprint(mean_absolute_error(model.predict(X_train),y_train))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train1,X_valid1,y_train1,y_valid1=train_test_split(X_c1,y_c1)\nX_train2,X_valid2,y_train2,y_valid2=train_test_split(X_c2,y_c2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#We select the optimal value of estimators and learning rate to get ultimate model.\n\nn_est=list(range(100,1500,200))\nrate=list(np.arange(0.01,0.11,0.02))\nvalue=list(itertools.product(n_est,rate))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"absolute={}\nabsolute1={}\nfor i in value:\n    model=XGBRegressor(n_estimators=i[0],learning_rate=i[1])\n    model.fit(X_train1, y_train1, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid1, y_valid1)],\n             verbose=False)\n    absolute[i]=mean_absolute_error(model.predict(X_valid1),y_valid1)\n    model.fit(X_train2, y_train2, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid2, y_valid2)],\n             verbose=False)\n    absolute1[i]=mean_absolute_error(model.predict(X_valid2),y_valid2)\nult=min(absolute,key=absolute.get)\nult1=min(absolute1,key=absolute1.get)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"For X_c1 - {} error is {} \\nFor X_c2- {} error is {} \".format(ult,absolute[ult],ult1,absolute1[ult1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ult_model=XGBRegressor(n_estimators=ult1[0],learning_rate=ult[1])\nult_model.fit(X_train2, y_train2, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid2, y_valid2)],\n             verbose=False)\nprint(mean_absolute_error(ult_model.predict(X_valid2),y_valid2))\nprint(mean_absolute_error(ult_model.predict(X_train2),y_train2))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"ult_model1=XGBRegressor(n_estimators=ult[0],learning_rate=ult[1])\nult_model1.fit(X_train1, y_train1, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid1, y_valid1)],\n             verbose=False)\nprint(mean_absolute_error(ult_model1.predict(X_valid1),y_valid1))\nprint(mean_absolute_error(ult_model1.predict(X_train1),y_train1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredictions=ult_model.predict(X_test_c2)\noutput=pd.DataFrame({'Id':test_id,'SalePrice':predictions})\noutput.to_csv('sample_submission.csv',index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}